---
title: Multi-test independent filtering for normal variates
author: Aaron Lun
output:
  BiocStyle::html_document:
    toc_float: yes
    fig_caption: no
---

```{r, echo=FALSE, results="hide"}
knitr::opts_chunk$set(message=FALSE, error=FALSE, warning=FALSE)
library(BiocStyle)
```

# Problem statement

We consider $m$ vectors that represent correlated sets of $n$ observations.
Each vector is of the form $\mathbf{y}_j = (Y_{j1}, Y_{j2}, ..., Y_{jn})^T$ where $Y_{jk}$ represent normally distributed random variables.
$Y_{1k}, Y_{2k}, ..., Y_{mk}$ are sampled from a multivariate normal (MVN) distribution with mean $\pmb\mu_k = (\mu_{1k}, ..., \mu_{mk})^T$ and covariance matrix $\pmb\Sigma$.
Sampling is independent for different $k$ involving separate MVN distributions, i.e., observations are independent across $k$ but not across $j$.
We use $\mathbf{Y}$ to denote the collection of these $m$ vectors, i.e., $\mathbf{Y} = [\mathbf{y}_1, ..., \mathbf{y}_m]$.

We want to filter on some function of all $\mathbf{\bar y}_j$ to decide whether or not to retain $\mathbf{Y}$.
The aim is to prove that any function of $\mathbf{\bar y}_j$ yields an independent filter statistic for linear models with an intercept term fitted to each $\mathbf{y}_j$.
This requires us to show that conditional distribution of variances and $p$-values (joint across all $j$) after filtering is the same as that before filtering.

# Proof

## Stage I

Let $\mathbf{z}_j = (z_{j1}, ..., z_{jn})^T$ where $z_{jk}$ are constants and $\sum_k z_{jk} = 0$. 
Define $m$ such vectors (i.e., $[\mathbf{z}_1, ..., \mathbf{z}_m]$), noting that the different vectors do not need to have the same constant values.
We define a function $Z(\mathbf{x})$ that takes some vector $\mathbf{x} = (x_1, ..., x_m)^T$ and returns $[\mathbf{z}^*_1, ..., \mathbf{z}^*_m]$ 
where $\mathbf{z}^*_j = \mathbf{z}_j + x_j\mathbf{1}$.
This is a collection of $m$ shifted vectors analogous to $\mathbf{Y}$.

For each vector $\mathbf{z}^*_j$ in the collection defined by $Z(\mathbf{x})$, 
we want to fit a linear model containing an intercept term that is not involved in forming the null hypothesis.
The set of model inferences across all $j$ are identical for all collections produced by the same $Z(.)$.
Intercept terms absorb any differences in $x_j$ so that the variances and $p$-values are the same for any choice of $\mathbf{x}$.

## Stage II

Collections of vectors are sampled from $\mathbf{Y}$ as described above.
For a vector $\mathbf{g} = (g_1, ..., g_m)^T$, the probability density of the $Z(\mathbf{g})$ collection is
$$
\mbox{det}(2\pi\pmb\Sigma)^{-n/2} \exp\left[ -\frac{1}{2} \sum_{k=1}^n (\mathbf{z}'_k + \mathbf{g} - \pmb\mu_k)^T \pmb\Sigma^{-1} (\mathbf{z}'_k + \mathbf{g} - \pmb\mu_k) \right]
$$
where $\mathbf{z}'_k = (z_{1k}, ..., z_{mk})^T$.
This can be re-interpreted as a probability density function (PDF) with respect to $\mathbf{g}$.
That is, we want the joint distribution of $\mathbf{g}$ corresponding to all collections that can be generated by $Z(.)$.
Under this interpretation, $\mathbf{g}$ follows a MVN distribution with mean $n^{-1}\sum_k \pmb\mu_k$ and covariance matrix $n^{-1}\pmb\Sigma$ (keeping in mind that $\sum_k z_{jk} = 0$).

We apply a filter on $Z(\mathbf{g})$ that involves (i) computing a scalar value from any arbitrary function $f(\bar{\mathbf{z}^*_1}, ..., \bar{\mathbf{z}^*_m})$,
and (ii) retaining only the instances of $Z(\mathbf{g})$ where the computed scalar is greater than a threshold $\tau$.
The first step corresponds directly to applying a filter on the scalar returned by $f(g_1, ..., g_m)$ at the same threshold.
The probability of passing this filter is constant for all $Z(.)$ as no $z_{jk}$ terms are involved in the expression for the PDF.

## Stage III

The sampling space of $\mathbf{Y}$ can be defined in terms of unique $Z(.)$.
Each $Z(.)$ corresponds to a subset of instances of $\mathbf{Y}$ that differ only by a set of additive factors that vary across $j$ but are constant across observations.
The subsets defined by different $Z(.)$ are mutually exclusive, and the union of subsets defines all possible $\mathbf{Y}$.

We now apply a filter to instances of $\mathbf{Y}$ based on the sample means of each of the $m$ vectors.
This discards the same proportion of instances from the subset defined by each $Z(.)$.
Recall that all instances in each subset have the same model statistics, i.e., variances and $p$-values.
If each subset loses the same proportion of instances, this implies that the filter will not change the distribution of statistics across subsets.

# Verification

We check the expected joint distribution of $\mathbf{g}$ with simulations.
First we define a function that determines which points in `mat` are consistent with a given $Z(.)$ (defined by `point`).

```{r}
onZ <- function(mat, point, tol=0.01) {
    point <- point - mean(point)
    adj <- t(t(mat) - point)

    # Only keeping points consistent with 'point + x' for some 'x'.
    ninst <- nrow(mat)
    is.max <- max.col(adj)
    is.max <- (is.max - 1) * ninst + 1:ninst
    is.min <- max.col(-adj)
    is.min <- (is.min - 1) * ninst + 1:ninst
    dev <- mat[is.max] - mat[is.min]

    list(
        keep=dev <= tol,
        g=rowMeans(adj)
    )
}
```

We define a wrapper function to simulate data and report statistics.

```{r}
library(mvtnorm)
simNcheck <- function(my.means, my.cov, points, tol=0.01) {
    nlibs <- nrow(my.means)
    ninst <- 1e6
    collected <- vector("list", nlibs)
    for (i in seq_len(nlibs)) {
        collected[[i]] <- rmvnorm(ninst, my.means[i,], sigma=my.cov)
    }

    ntests <- ncol(my.means)
    all.on <- vector("list", ntests)
    all.g <- vector("list", ntests)
    for (j in seq_len(ntests)) {
        current <- vector("list", nlibs)
        for (i in seq_len(nlibs)) {
            current[[i]] <- collected[[i]][,j]
        }
        current <- do.call(cbind, current)

        mapped <- onZ(current, points[,j], tol=tol)
        all.on[[j]] <- mapped$keep
        all.g[[j]] <- mapped$g
    }

    # Keeping only those that are consistent with 'points'.
    keep <- Reduce("&", all.on)
    all.g <- lapply(all.g, "[", keep)

    observed.cov <- matrix(0, ntests, ntests)
    for (j in seq_len(ntests)) {
        for (k in seq_len(j)) {
            observed.cov[j,k] <- 
                observed.cov[k,j] <- 
                cov(all.g[[j]], all.g[[k]])
        }
    }

    list(
        n=sum(keep),
        observed.mean=vapply(all.g, mean, 0),
        observed.cov=observed.cov,
        intended.mean=colMeans(my.means),
        intended.cov=my.cov/nlibs
    )
}
```

We observe consistent results for a number of scenarios, which provides some validation of our proof.

```{r}
set.seed(34234)

simNcheck(
    my.means=rbind(0:1, 2:1, 0, 1:0),
    my.cov=rbind(c(1, 0.5), c(0.5, 1)),
    points=cbind(c(0, 0, 1, 0), c(1, 2, 0, 0))
)

simNcheck(
    my.means=rbind(0:1, c(0.5, 4), 0, 1:0),
    my.cov=rbind(c(1, 0), c(0, 1)),
    points=cbind(c(0, 2, 4, 0), c(1, 2, 0, 0))
)

simNcheck(
    my.means=rbind(0:1, c(0.5, 1), c(0, 0.5)),
    my.cov=rbind(c(0.5, 0.2), c(0.2, 1)),
    points=cbind(c(0, 1, 0), c(1, 0, 0))
)

simNcheck(
    my.means=rbind(0:1, 1:0, 0, 0),
    my.cov=rbind(c(1, -0.25), c(-0.25, 1)),
    points=cbind(c(0, 1, 0, 0), c(1, 0, 0, 0))
)

simNcheck(
    my.means=rbind(0:2, 1:3),
    my.cov=rbind(c(  1, 0.2, 0.3),
                 c(0.2, 0.5, 0.6),
                 c(0.3, 0.6,   2)),
    points=cbind(c(-1, 1), c(1, 0), c(0, -1))
)
```

# Comments

This proof extends the result for the special single-vector case (i.e., with $m=1$).
Any function $f(.)$ of the sample means can be used for filtering.
This includes functions that ignore some $j$ altogether, operate on differences between sample means, take the maximum or minimum of the sample means across $j$, etc.
Consideration of correlated tests is arguably necessary in real biological systems involving regulatory relationships between features of interest.

One might wonder whether the above work is strictly necessary.
It might seem obvious that, if the sample mean was an independent filter statistic for an individual vector, 
a collection of sample means would be independent for a collection of vectors even if those vectors were correlated.
However, it is easy to demonstrate that this is not the case.
For example, the sign of the test result is independent of the $p$-value for single null hypotheses but not for a collection of correlated null hypotheses.

```{r}
set.seed(1000)
y0 <- matrix(rnorm(100000), ncol=10)
y1 <- y0 + matrix(rnorm(100000), ncol=10)
y2 <- y0 + matrix(rnorm(100000), ncol=10)

library(limma)
design <- model.matrix(~gl(2,5))

fit1 <- lmFit(y1, design)
fit1 <- eBayes(fit1)
res1 <- topTable(fit1, sort.by="none", n=Inf)

fit2 <- lmFit(y2, design)
fit2 <- eBayes(fit2)
res2 <- topTable(fit2, sort.by="none", n=Inf)

keep <- sign(res1$logFC)==sign(res2$logFC)
par(mfrow=c(1,2))
hist(c(res1$P.Value, res2$P.Value), main="Full", col="grey80")
hist(c(res1$P.Value[keep], res2$P.Value[keep]), main="Filtered", col="grey80")
```

# Session information

```{r}
sessionInfo()
```

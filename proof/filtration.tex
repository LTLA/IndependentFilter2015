\documentclass{article}
\usepackage[margin=3cm]{geometry}
\usepackage{amsmath}
\usepackage{natbib}

\newcommand{\nsamples}{n}
\newcommand{\sample}{i}
\newcommand{\gene}{j}

\newcommand{\normal}{\mathcal{N}}
\newcommand{\normean}{\mu}
\newcommand{\normvar}{\sigma^2}

\newcommand\design{\mathbf{X}}
\newcommand{\nullhypo}[1]{H_{0#1}}
\newcommand\deviance{D}
\newcommand{\normvarhat}{\hat\sigma^2}
\newcommand{\fitvec}{\hat\meanvec}
\newcommand{\modifier}{\gamma}
\newcommand{\residdf}{d}
\newcommand{\npredictors}{m}
\newcommand{\fstat}{F}
\newcommand\testdf{t}
\newcommand{\priorvar}{\normvar_0}
\newcommand{\normvarshrunk}{\tilde\sigma^2}
\newcommand{\priordf}{\residdf_0}

\newcommand{\logcount}{z}
\newcommand{\oriobs}[0]{\mathbf{\logcount}}
\newcommand{\modobs}[0]{\mathbf{\logcount}'}
\newcommand{\instance}[0]{\mathbf{\logcount}^*}
\newcommand{\filter}[0]{\mathbf{\logcount}_0}
\newcommand{\allones}[0]{\mathbf{1}}

\newcommand{\obsmat}[0]{\mathbf{Z}}
\newcommand{\matinst}[0]{\mathbf{Z^*}}
\newcommand{\modmat}[0]{\mathbf{Z}'}

\newcommand{\test}{l}
\newcommand{\ntests}{L}
\newcommand{\scaling}{A}
\newcommand{\interceptor}{B}
\newcommand{\scalar}{\scaling_\test}
\newcommand{\intercept}{\interceptor_{\sample\test}}
\newcommand{\meanintercept}{\bar{\interceptor_\test}}
\newcommand{\testvar}{\normvar_\test}

\newcommand{\dt}{\modifier_\test}

\newcommand{\matlogcount}{\logcount^*_{\sample\test}}
\newcommand{\internalQ}{Q_\sample}
\newcommand{\internalR}{R}
\newcommand{\internalP}{P_\sample}

\newcommand{\invertvar}{\sigma^{-2}}
\newcommand{\inverttestvar}{\sigma^{-2}_\test}

\newcommand{\testone}{\test_1}
\newcommand{\testtwo}{\test_2}
\newcommand{\scalarone}{\scaling_{\testone}}
\newcommand{\interceptone}{\interceptor_{\sample\testone}}
\newcommand{\scalartwo}{\scaling_{\testtwo}}
\newcommand{\intercepttwo}{\interceptor_{\sample\testtwo}}
\newcommand{\dtone}{\modifier_{\testone}}
\newcommand{\dttwo}{\modifier_{\testtwo}}
\newcommand{\testvarone}{\normvar_{\testone}}
\newcommand{\testvartwo}{\normvar_{\testtwo}}
\newcommand{\matlogcountone}{\logcount^*_{\sample\testone}}
\newcommand{\matlogcounttwo}{\logcount^*_{\sample\testtwo}}

\newcommand{\testcolumn}[0]{\oriobs_\test}
\newcommand{\testinstance}[0]{\instance_\test}
\newcommand{\testinstone}[0]{\instance_{\testone}}
\newcommand{\testinsttwo}[0]{\instance_{\testtwo}}
\newcommand{\constant}{C}

\newcommand{\testmodlimit}{\modifier_{0\test}}
\newcommand{\testthreshold}{\threshold_{(\test)}}
\newcommand{\testfilter}{\oriobs_{0\test}}

\newcommand{\nnbiid}{\upsilon}
\newcommand{\threshold}{\tau}

\begin{document}

\section{Independence of the overall mean with normality}

\subsection{Description of the problem}
Consider a vector of observations $\oriobs = (\logcount_1, \logcount_2, ..., \logcount_\nsamples)^T$.
Each $\logcount_\sample$ represents the observation for sample $\sample$ and is independently sampled from a $\normal(\normean_\sample, \normvar)$ distribution with mean $\normean_\sample$ and variance $\normvar$. 
Assume that a linear model is fitted to $\oriobs$ using a design matrix $\design$ that contains an intercept term, i.e., one column of the matrix that is set to unity for all samples.
Let the null hypothesis $\nullhypo{}$ be defined in a manner that does not involve the intercept term.
Now, consider the sample variance, test statistic and $p$-value against $\nullhypo{}$.
The aim is to prove that the distribution of each of these statistics is not altered after retaining only the instances of $\oriobs$ where the sample mean $\bar{\oriobs} = (\logcount_1+\logcount_2+...+\logcount_\nsamples)/\nsamples$ is greater than some threshold.
In short, prove that the sample mean is an independent filter statistic.

% Assumes that design matrices, contrasts and (generalized) linear modelling 
% have already been explained.

% Note that the same properties are described by Bourgon \emph{et al.} in their 2010 paper
% \cite{bourgon2010}. 

\subsection{Notation for various statistics in the linear model}
Let $\deviance$ denote the total residual deviance of a linear model, fitted to a particular instance of $\oriobs$. 
The sample variance for this instance is estimated by computing 
\[
\normvarhat = \frac{\deviance}{\residdf}
\]
where $\residdf$ is the residual d.f., i.e., $\residdf=\nsamples-\npredictors$ for $\npredictors$ columns in $\design$.
Now, consider the design matrix corresponding to $\nullhypo{}$.
Let $\npredictors_0$ be the number of columns for the null design matrix, and let $\deviance_0$ be the total deviance of the null fit.
The F-statistic against $\nullhypo{}$ is defined as 
\[
\fstat = \frac{\deviance_0 - \deviance}{\normvarhat\testdf} 
\]
where $\testdf = \npredictors - \npredictors_0$.
In the standard F-test, the $p$-value is defined as the upper tail probability of the F-distribution on $\testdf$ and $\residdf$ d.f.'s at the computed value of $\fstat$.
For the moderated F-test, information is shared between different tests (i.e., genes or regions) to stabilize the variance estimates.
Here, a shrunken variance is defined as 
\[
\normvarshrunk = \frac{\normvarhat\residdf + \priorvar\priordf}{\residdf + \priordf}
\]
where the scaling factor $\priorvar$ and the prior d.f. $\priordf$ are estimated by fitting $\priorvar F( \residdf, \priordf)$ to the distribution of $\normvarhat$ across all tests.
This shrunken estimate is used in place of $\normvarhat$ to compute a moderated equivalent to $\fstat$.
The corresponding $p$-value is defined as the tail probability for a F-distribution on $\testdf$ and $\residdf + \priordf$ d.f.'s.

\subsection{Proof}
\label{sec:single_filter_proof}
Consider a single instance of $\oriobs$, denoted as $\instance = (\logcount^*_1, \logcount^*_2, ..., \logcount^*_\nsamples)^T$. 
Fit a linear model to $\instance$ using $\design$. 
Denote the resulting vector of fitted values as $(\hat\normean_1, \hat\normean_2, ..., \hat\normean_\nsamples)^T$.
Let $\logcount'_\sample = \logcount^*_\sample + \modifier$ for a real scalar $\modifier$, such that $\modobs= \oriobs+\modifier\allones$ defines a line in the $\nsamples$-dimensional space with respect to variable $\modifier$.
At any point on $\modobs$, the corresponding value of $\modifier$ will be directly absorbed by the intercept coefficient in the fitted model.  
This is due to the use of an identity link such that any changes in the observations will result in an equivalent change in the relevant coefficient.
As a result, the fitted value for each sample will increase by $\modifier$, i.e., $\hat\normean'_\sample = \hat\normean_\sample + \modifier$.  
The total residual deviance of the fit does not depend on $\modifier$, as 
\[
\deviance = 2 \sum_{\sample=1}^\nsamples ( \logcount^*_\sample - \hat\normean_\sample)^2
= 2 \sum_{\sample=1}^\nsamples ( \logcount^*_\sample + \modifier - \hat\normean_\sample - \modifier)^2
= 2 \sum_{\sample=1}^\nsamples ( \logcount^*_\sample - \hat\normean_\sample)^2 \;.
\]
This means that $\deviance$ will be the same for all points on $\modobs$.  
Now, $\nullhypo{}$ is defined such that the null design matrix can be parameterized with an intercept term.
Thus, the total residual deviance under the null (i.e., $\deviance_0$) will also be identical throughout $\modobs$.
This means that the sample variances, test statistics and $p$-values will be constant for all points on $\modobs$.

% Probably can generalize this to non-null hypotheses as well. You can have
% differing 'mu' and it'll still work. Could do simulations to check.

The entire $\nsamples$-dimensional space can be treated as the infinite set of all unique $\modobs$ lines, e.g., by considering all lines derived from instances where $\logcount^*_1=0$ and all other $\logcount^*_\sample$ values are varied.
Each instance of $\oriobs$ can be treated as a point on one of these lines.
For a set of all points on a given line, the application of a filter will remove a proportion of these points.
The aim of an independent filter is to remove the same proportion of points from each line. 
This ensures that the distribution of each relevant statistic (e.g., sample variance, test statistic or $p$-value) will not be changed by filtering.
More formally, for any value of a given statistic, the probability density can be defined as the sum of densities across all lines associated with that value.
This is possible because all points on a line are associated with a single value of each statistic.
If a filter removes the same proportion of points from each line, the sum of densities will decrease by the same fold-change for all values of the statistic.
This means that the relative probability density between different values will not change, i.e., the distribution of that statistic will be the same after filtering.

To identify this independent filter, the distribution of points within each line must be determined. 
The probability density function across any given $\modobs$ can be written as
\[
f(\modobs | \instance, \normean_1, ..., \normean_\nsamples, \normvar) \propto \exp\left[-\frac{1}{2\normvar} \sum_{\sample=1}^\nsamples (\logcount^*_\sample + \modifier - \normean_\sample)^2 \right] \;,
\]
where the proportionality constant has no dependence on $\modifier$. 
This represents a normal distribution when considered with respect to $\modifier$, with a mean of $\bar\normean - \bar{\instance}$ and a variance of $\normvar/\nsamples$. 
Note that the term $\bar\normean$ is simply defined as the mean of $\normean_\sample$ over all samples $\sample$.
Now, consider the proportion of points on $\modobs$ for which $\modifier < \modifier_0$, where 
\[
 \modifier_0 = \bar\normean - \bar{\instance} + \threshold
\]
for some constant $\threshold$. 
This proportion can be defined as the cumulative probability of the $\normal(\bar\normean - \bar{\instance}, \normvar/\nsamples)$ distribution below $\modifier_0$.
It is constant for each $\modobs$ line as the value of $\normvar$ is constant, and the difference between $\modifier_0$ and the distribution mean is always $\threshold$.
Thus, removal of all points with $\modifier < \modifier_0$ on each line will constitute an independent filter.

For a more intuitive interpretation, the location of the filter threshold $\modifier_0$ on each $\modobs$ line can projected back into $\nsamples$-dimensional space.  
Consider some point $\filter= (\logcount_{01}, \logcount_{02}, ..., \logcount_{0\nsamples})^T$ where $\filter=\instance+\modifier_0\allones$.
The shape of the filter boundary in the $n$-dimensional space can be determined by considering the sum of all elements in $\filter$.  
This gives
\[
\logcount_{01} + \logcount_{02} + ... + \logcount_{0\nsamples} = \nsamples(\bar\normean + \threshold - \bar{\instance}) + \sum_{\sample=1}^\nsamples \logcount^*_\sample = \nsamples(\bar\normean + \threshold) \;,
\]
i.e., the points corresponding to $\modifier_0$ across all $\modobs$ form an affine hyperplane. 
Removal of all points where $\modifier \le \modifier_0$ is equivalent to the removal of all points ``under'' this plane. 
Specifically, an instance is removed if the sum of observations is less than $\nsamples(\bar\normean+\threshold)$ or the sample mean is less than $\bar\normean + \threshold$, for some arbitrarily chosen value of $\threshold$.  
This means that the sum or sample mean is an independent filter for normally distributed values.

% It should be stressed that i.i.d.  normality only refers to the
% distribution of observations for a single test/feature. It does not refer to
% the distribution of observations across multiple tests/features for a
% genome-wide experiment. Different means can be easily accommodated with an offset.

\subsection{Further comments on filter behaviour}
% Assuming shrinkage to a common value, which is default in eBayes.

The independence result is not limited to cases where the null hypothesis is true.
The proof does not place any restrictions on the possible values for $\normean_\sample$ across the different samples.
Thus, the sample mean will still be an independent filter in situations where the null is false, i.e., filtering will not change the distribution of the sample variances or test statistics.
This is important as retained false nulls will still be used in EB shrinkage.
Any distortions of the sample variance distribution for the false nulls will affect the shrunken variances of true nulls.
Thus, filter independence for the false nulls is still necessary to maintain type I error control among the true nulls.

One assumption of the independence result is that the parametrizations of the full and null design matrices must contain intercept terms.  
This may not be possible for some designs or contrasts, in which case the sample mean may no longer be independent of the $p$-value.  

Another issue is that independence does not strictly hold in the presence of heteroskedasticity.
Consider a set of genes with observations sampled from a $\normal(0, \normvar_\gene)$ distribution, where $\normvar_\gene$ is different for each gene $\gene$.
Filtering to retain genes with large overall means will enrich for genes with greater $\normvar_\gene$.
This is because the overall means for such genes are more variable, with a variance of $\normvar_\gene \nsamples^{-1}$.
These genes are more likely to obtain extreme (large) values for the overall means, leading to retention.
Distortions in the variance distribution after filtering will subsequently alter the behaviour of EB shrinkage.

%The issue is a bit more complex with \texttt{voom}'d counts and precision
%weights. The $t$-statistic lines can be perturbed if the weights do not change
%proportionally for all observations with decreasing $\delta$. The effect on the
%probability distribution within each line is even less clear. This is problematic
%if there is a strong mean-variance trend in the data. I was going to mention it
%but I really don't use voom anywhere else in the paper. 

\section{Extending the proof for multiple correlated tests}

\subsection{Overview}
In genome-wide analyses, the outcome of a test for one genomic feature is likely to be correlated to another that for another feature.
This is particularly relevant for ChIP-seq data where positive correlations will be present between overlapping windows.
Biological correlations may also be present between genes that are in the same regulatory pathway.
For practical applications, the chosen filter statistic must remain independent of the $p$-value in the presence of dependencies between tests.
This section aims to show that this is the case for the sample mean with correlated normal observations.
Again, normality is used here for simplicity, given the difficulty of obtaining proofs with the NB distribution.

\subsection{Description of the problem}
Consider a scenario involving $\ntests$ correlated tests.
Let $\obsmat$ be a $\nsamples \times \ntests$ matrix where $\logcount_{\sample\test}$ is the observation for sample $\sample$ in test $\test$.
Each column of $\obsmat$ can be treated as a vector of observations $\testcolumn$ for test $\test$.
Assume that $\logcount_{\sample\test} \sim \normal(\scalar\normean_\sample + \intercept, \testvar)$ where $\normean_\sample \sim \normal(0, \normvar)$ for constant $\normvar$.
$\scalar$ and $\testvar$ are the same for all observations in each $\test$, whereas $\intercept$ is an observation-specific constant.
This scheme introduces correlations between the corresponding observations of the $\ntests$ tests.
Filtering is performed by only retaining instances of $\obsmat$ when the sample mean $\bar{\testcolumn}$ is above some threshold value for all tests $\test = 1, ..., \ntests$.

% It shouldn't matter that we have a central mean of zero, because intercept will just increase everything up again.
% To wit; any submean that is generated with a non-zero central mean, you can mimic by finding the point with the same 
% probability density under a zero central mean and increasing the intercepts across all tests.

Assume that a linear model is fitted to the $\testcolumn$ for each $\test$ using a design matrix that contains an intercept term.
The fitted model for each $\testcolumn$ is used to compute a sample variance for each test.
A $p$-value is also computed against an appropriate $\nullhypo{}$, i.e, one that does not involve the intercept.
Now, consider the joint distribution of $p$-values (or sample variances, or test statistics) for all tests, using only the retained instances of $\obsmat$.
The aim is to prove that the joint distribution of each statistic is the same as that prior to filtering.
If so, the sample means will be independent filter statistics for correlated tests.

\subsection{Proof}
Let $\matlogcount$ be the observation for sample $\sample$ of test $\test$ in $\matinst$, i.e., an instance of $\obsmat$.
Define the $\nsamples \times \ntests$ matrix $\modmat$ where the entry $(\sample, \test)$ is defined as $\matlogcount + \dt$ for a real scalar $\dt$.
A different value of $\dt$ is used for each $\test$.
This forms a hyperplane across the $\nsamples\ntests$-dimensional space when each $\dt$ is allowed to vary independently.
The deviances of the fitted full and null models do not depend on $\dt$ as any changes are absorbed by the intercept term.
This means that the sample variance for test $\test$ will be constant for all points on the hyperplane (though the value can still differ between tests).
Similar reasoning can be applied to the $p$-value for any given test.
Now, consider the probability density of points throughout this hyperplane. 
Under the dependence structure defined above, this can be written as
\begin{align*}
& f(\modmat | \matinst, \scaling_1, ..., \scaling_\ntests, \interceptor_{11}, ..., \interceptor_{\nsamples\ntests}, \normvar, \normvar_1, ..., \normvar_\ntests) \\
& \quad\propto \prod_{\sample=1}^\nsamples \left( \int_{-\infty}^{\infty} \exp\left[ - \frac{\normean_\sample^2}{2\normvar} 
	- \sum_{\test=1}^\ntests \frac{(\matlogcount + \dt - \scalar\normean_\sample - \intercept )^2}{2\testvar} \right]\mbox{d}\mu_\sample \right) 
\end{align*}
where the proportionality constant has no dependence on $\dt$.  
The exponent inside the integral for each sample $i$ can be rewritten in the form of $-(P_i + Q_i)/2$ where
\begin{align*}
\internalP &= \internalR\left[ \normean_\sample - \frac{1}{\internalR}\sum_\test \frac{\scalar(\matlogcount + \dt - \intercept)}{\testvar} \right]^2 \quad\mbox{ with } \internalR = \invertvar + \sum_\test \scalar^2\inverttestvar \;, \mbox{ and }\\
\internalQ &= \frac{1}{\internalR} \left[ \sum_\test \frac{(\matlogcount + \dt - \intercept)^2}{\testvar} (\internalR - \scalar^2 \invertvar_\test) \right. \\
    &\quad \left. - \sum_{\testone} \sum_{\testtwo} \frac{2 \scalarone\scalartwo (\dtone + \matlogcountone - \interceptone)(\dttwo + \matlogcounttwo - \intercepttwo)}{\testvarone\testvartwo} \right] \;.\end{align*}
Sums are computed over all $\test$ or all $(\testone, \testtwo)$, i.e., all unique pairs of tests where $\testone > \testtwo$.
$\internalP$ represents the negative exponent of the normal PDF with respect to $\normean_\sample$.
Taking the definite integral of $\exp(-\internalP/2)$ across each $\normean_\sample \in (-\infty, \infty)$ will yield a constant term containing only $\scalar$, $\normvar$ and $\testvar$. 
This can be absorbed into the proportionality constant, leaving
\[
f(\modmat | \matinst, \scaling_1, ..., \scaling_\ntests, \interceptor_{11}, ... \interceptor_{\nsamples\ntests}, \normvar, \normvar_1, ..., \normvar_\ntests) 
	\propto \exp\left(-\frac{1}{2} \sum_{\sample=1}^\nsamples \internalQ \right) \;.
\]

Examination of the above expression indicates that the points on the hyperplane follow a multivariate normal (MVN) distribution with respect to all $\dt$. 
Consider that
\begin{align*}
\sum_\sample \internalQ &= \frac{1}{\internalR} \left( \sum_\test \left[ (\internalR - \scalar^2 \inverttestvar) \sum_\sample \frac{(\dt - \intercept + \matlogcount)^2}{\testvar} \right] \right. \\
    & \quad - \left. \sum_{\testone} \sum_{\testtwo} \left[ \sum_\sample \frac{2\scalarone\scalartwo(\dtone + \matlogcountone - \interceptone)(\dttwo + \matlogcounttwo - \intercepttwo)}{\testvarone\testvartwo} \right] \right)
\end{align*}
where the sum of $\internalQ$ is computed over all $\sample$.
Let $\testinstance$ represent the instance of $\testcolumn$ for each test $\test$ within $\matinst$.
The summation above can be further rewritten as
\begin{align*}
\sum_i \internalQ &= \constant + \frac{1}{\internalR} \left( \sum_\test \left[ (\internalR - \scalar^2 \inverttestvar) \frac{\nsamples (\dt - \meanintercept + \bar{\testinstance})^2}{\testvar} \right] \right. \\
    &\quad - \left. \sum_{\testone} \sum_{\testtwo} \left[ \frac{2\scalarone\scalartwo\nsamples(\dtone - \bar{\interceptor_{\testone}} + \bar{\testinstone})(\dttwo - \bar{\interceptor_{\testtwo}} + \bar{\testinsttwo})}{\testvarone\testvartwo} \right] \right) 
\end{align*}
where $\constant$ is some constant that does not depend on any $\dt$, and $\meanintercept$ is the mean of $\intercept$ across all $\sample$ for a given $\test$.
This above expression is twice the negative exponent of the PDF of a MVN distribution with mean $\meanintercept - \bar{\testinstance}$ for each $\test$, i.e., a mean vector of 
\[
(\bar{\interceptor_1}-\bar{\instance_1}, \bar{\interceptor_2} - \bar{\instance_2}, ..., \bar{\interceptor_\ntests} - \bar{\instance_\ntests})^T \;.
\]
Similarly, the inverse of the covariance matrix for this distribution can be written as
\[
\frac{\nsamples}{\internalR} \begin{pmatrix}
(\internalR - \scaling_1^2\invertvar_1)\invertvar_1 & -\scaling_1\scaling_2(\normvar_1\normvar_2)^{-1} & ... & -\scaling_1\scaling_\ntests(\normvar_1\normvar_\ntests)^{-1} \\
-\scaling_2\scaling_1(\normvar_2\normvar_1)^{-1} & (R - \scaling_2^2\invertvar_2)\invertvar_2 & ... & ... \\
   ... & ... & ... & ... \\
-\scaling_\ntests\scaling_1(\normvar_\ntests\normvar_1)^{-1} & ... & ... & (\internalR - \scaling_\ntests^2\invertvar_\ntests)\invertvar_\ntests
\end{pmatrix} \; .
\]

The $\nsamples\ntests$-dimensional space can be parameterized as an infinite set of hyperplanes. 
Define the filter boundary as a continuous function that partitions a given hyperplane into two spaces, such that the points are only retained in one space.
An independent filter will remove the same proportion of points from each hyperplane.
This ensures that the distribution of points between hyperplanes is unchanged after filtering.
Recall that all points on a hyperplane will have the same $p$-value, test statistic and sample variance for a given test.
If the distribution of points between hyperplanes does not change, the distribution of each statistic across all retained points will also be unchanged.
This is a multi-dimensional extension of the reasoning in Section~\ref{sec:single_filter_proof} for the $\ntests = 1$ case. 

To obtain an independent filter, consider the distribution of points within each hyperplane.
Only the location of the MVN changes when a different $\matinst$ is used to define $\modmat$.
Thus, the filter boundary must have the same relative position from the MVN mean in each hyperplane.
A simple independent filter is derived as follows.
Remove all points on the $\modmat$ hyperplane where $\dt \le \testmodlimit$ for any $\test$, where $\testmodlimit = \meanintercept -\bar{\testinstance} + \testthreshold$ for some constant $\testthreshold$.
This constitutes an independent filter as the closest distance from the MVN mean to the filter boundary in any hyperplane is always $\testthreshold$ for test $\test$.
Using the logic from the univariate case, one can project the filter boundary for $\dt$ on the $\modmat$ hyperplane back to the $\nsamples\ntests$-dimensional space.
Let $\testfilter = \testinstance + \testmodlimit\allones$, such that the boundary is defined as
\[
\bar{\testfilter} = \meanintercept + \testthreshold
\]
for each test $\test$.
In other words, the sample mean for each test in a given instance of $\obsmat$ must be greater than or equal to $\meanintercept + \testthreshold$ in order to retain that instance.
This result demonstrates that the sample mean is still an independent filter statistic when the tests are correlated.
Of course, more complex functions can be used as the filter boundary so long as the location of the function shifts with the MVN mean between $\modmat$ hyperplanes.

% It's also possible to require, say, at least 'x' tests above the threshold. You can just 
% think of the proportion of the probability density in each hyperplane that passes, which
% should be the same if the thresholds hop along.

\section{Conclusion}
This section demonstrates that the sample mean is an independent filter statistic of the $t$-test.
The independence of the sample mean also holds in the presence of dependencies between the means of different tests.
The hope is that analogous behaviour is observed in the equivalent statistic for NB models typically used for count data.
One candidate for the equivalent statistic would be the maximum likelihood estimate of the NB mean.

\end{document}

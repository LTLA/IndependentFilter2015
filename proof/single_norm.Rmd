---
title: Single-test independent filtering for normal variates
author: Aaron Lun
output:
  BiocStyle::html_document:
    toc_float: yes
    fig_caption: no 
---

```{r, echo=FALSE, results="hide"}
knitr::opts_chunk$set(message=FALSE, error=FALSE, warning=FALSE)
library(BiocStyle)
```

# Problem statement

Consider a random vector of observations $\mathbf{y} = (Y_1, Y_2, ..., Y_n)^T$ where $Y_i$ are independent random variables following $\mathcal{N}(\mu_i, \sigma^2)$ distributions.
We filter to retain only the $\mathbf{y}$ where the sample mean $\mathbf{\bar y}$ is greater than some constant threshold $\tau$.
The aim is to prove that $\mathbf{\bar y}$ is an independent filter statistic for linear models with an intercept term.
This requires us to show that conditional distribution of variances and $p$-values after filtering is the same as that before filtering.

A proof of the same result is described in the supplementary materials of the [Bourgon paper](https://doi.org/10.1073/pnas.0914005107), 
though I find it very opaque; hence the alternative shown here.

# Proof

## Stage I

Let $\mathbf{z} = (z_1, ..., z_n)^T$ where $z_i$ are constants and $\sum_i z_i = 0$.
We define $Z(x) = \mathbf{z} + x\mathbf{1}$, a function that takes some scalar $x$ and returns shifted vectors.
For example, $Z(x_1)$ and $Z(x_2)$ yield vectors that only differ by the addition of a constant ($x_1 - x_2$) to all observations.

For each vector, we want to fit a linear model containing an intercept term that is not involved in forming the null hypothesis.
Model inferences are identical for all vectors produced by the same $Z(.)$.
This is because the intercept term absorbs any differences in $x$, which means that the variances and $p$-values are the same for any choice of $x$.

## Stage II

Vectors are sampled from $(Y_1, ..., Y_n)^T$ as above.
The probability density of the $Z(\gamma)$ vector is
$$
\frac{1}{\sigma\sqrt{2\pi}} \exp\left[ \frac{1}{2\sigma^2} \sum_{i=1}^n (\gamma + z_i - \mu_i)^2 \right ]\; .
$$
This can be re-interpreted as a probability density with respect to $\gamma$.
That is, we want the distribution of $\gamma$ for all vectors that can be generated by $Z(.)$.
Under this interpretation, $\gamma$ would follow a $\mathcal{N}(n^{-1}\sum \mu_i, \sigma^2n^{-1})$ distribution (keep in mind that the sum of $z_i$ is zero).

Consider applying a filter on $Z(\gamma)$ that requires the sample mean to be greater than a threshold $\tau$.
This corresponds directly to a filter on $\gamma$ at the same threshold.
The probability of obtaining $\gamma > \tau$ is constant for all $Z(.)$ as no $z_i$ terms are involved in the above expression.

## Stage III

The sampling space of $\mathbf{y}$ can be defined in terms of unique $Z(.)$.
Each $Z(.)$ corresponds to a subset of instances of $\mathbf{y}$ that differ only by a constant scaling factor.
The subsets defined by different $Z(.)$ are mutually exclusive, and the union of subsets defines all possible $\mathbf{y}$.

We now apply a sample mean threshold to instances of $\mathbf{y}$, which discards the same proportion of instances from the subset defined by each $Z(.)$.
Recall that all instances in each subset have the same model statistics, i.e., variances and $p$-values.
If each subset loses the same proportion of instances, this implies that the filter will not change the distribution of statistics across subsets.

# Comments

## Supported types of hypotheses

The proof does not place any restrictions on the possible values for $\mu_i$ across the different samples.
Thus, the sample mean will still be an independent filter in situations where the null is false, 
i.e., filtering will not change the distribution of the sample variances or test statistics.
This is important as retained false nulls will still be used in empirical Bayes shrinkage in `r Biocpkg("limma")`.
Any distortions of the sample variance distribution for the false nulls will affect the shrunken variances of true nulls.
Thus, filter independence for the false nulls is still necessary to maintain type I error control among the true nulls.

One assumption of the proof is that the parametrizations of the full and null design matrices must contain intercept terms.
This may not be possible for some designs or contrasts, in which case the sample mean may no longer be independent of the $p$-value.
To demonstrate:

```{r}
set.seed(1000)
y <- matrix(rnorm(1000000), ncol=10)
X <- cbind(1:10 - 5.5)

library(limma)
full <- lmFit(y, X)
full <- eBayes(full)

filt.stat <- rowMeans(y)
keep <- head(order(filt.stat, decreasing=TRUE), 10000)
filtered <- lmFit(y[keep,], X)
filtered <- eBayes(filtered)
    
par(mfrow=c(1,2))
hist(full$p.value, main="Full", col="grey80")
hist(filtered$p.value, main="Filtered", col="grey80")
```

## Dealing with heteroskedasticity

Another issue is that independence does not strictly hold in the presence of heteroskedasticity.
The proof fails for vectors that exhibit differences in variance between observations.
Filtering on the weighted mean does not do much better.
This is probably because a weighted linear model is transformed into a standard linear model by multiplying both sides by the square root of the weights - 
this eliminates the intercept from the model, resulting in the same problem described above.

```{r, fig.wide=TRUE}
set.seed(1000)
v <- 1:10
y <- matrix(rnorm(1000000, sd=sqrt(v)), ncol=10, byrow=TRUE)
X <- model.matrix(~gl(2, 5))

library(limma)
full <- lmFit(y, X, weight=1/v)
full <- eBayes(full)

filt.stat <- rowMeans(y)
keep <- head(order(filt.stat, decreasing=TRUE), 10000)
filtered <- lmFit(y[keep,], X, weight=1/v)
filtered <- eBayes(filtered)
   
par(mfrow=c(1,3))
hist(full$p.value, main="Full", col="grey80")
hist(filtered$p.value, main="Filtered", col="grey80")

wfit <- lmFit(y, w=1/v)
keep <- head(order(wfit$coefficients, decreasing=TRUE), 10000)
filtered <- lmFit(y[keep,], X, weight=1/v)
filtered <- eBayes(filtered)
hist(filtered$p.value, main="Filtered (weights)", col="grey80")
```

Another aspect of heteroskedasticity is that of differences in variance between features (e.g., genes) in high-throughput data analysis.
Filtering to retain features with large sample means will enrich for genes with greater variance.
This is because the sample means for such features are more variable and more likely to obtain extreme (large) values.
Distortions in the variance distribution after filtering will subsequently alter the behaviour of EB shrinkage.

# Session information

```{r}
sessionInfo()
```
